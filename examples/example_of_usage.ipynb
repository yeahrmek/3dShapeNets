{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import.\n",
    "\n",
    "Note, that current version of RBM works with neon version 1.1.3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../dbn/')\n",
    "\n",
    "\n",
    "from rbm_layer import ConvolutionalRBMLayer, RBMLayerWithLabels, RBMLayer\n",
    "from rbm import RBM\n",
    "from rbm_optimizer import GradientDescentMomentumRBM\n",
    "\n",
    "\n",
    "from neon.backends import gen_backend\n",
    "from neon.optimizers import MultiOptimizer\n",
    "from neon.initializers import GlorotUniform\n",
    "from neon.data import DataIterator\n",
    "from neon.data.datasets import fetch_dataset\n",
    "from neon.data.datasets import dataset_meta\n",
    "from neon.callbacks.callbacks import Callbacks\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "import os\n",
    "import gzip\n",
    "import cPickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.  Function to load MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_mnist(path=\".\", normalize=True):\n",
    "    \"\"\"\n",
    "    Fetch the MNIST dataset and load it into memory.\n",
    "\n",
    "    Args:\n",
    "        path (str, optional): Local directory in which to cache the raw\n",
    "                              dataset.  Defaults to current directory.\n",
    "        normalize (bool, optional): whether to scale values between 0 and 1.\n",
    "                                    Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Both training and test sets are returned.\n",
    "    \"\"\"\n",
    "    mnist = dataset_meta['mnist']\n",
    "\n",
    "    filepath = os.path.join(path, mnist['file'])\n",
    "    if not os.path.exists(filepath):\n",
    "        fetch_dataset(mnist['url'], mnist['file'], filepath, mnist['size'])\n",
    "\n",
    "    with gzip.open(filepath, 'rb') as mnist:\n",
    "        (X_train, y_train), (X_test, y_test) = cPickle.load(mnist)\n",
    "        # X_train = X_train.reshape(-1, 1, 28, 28)\n",
    "        # X_test = X_test.reshape(-1, 1, 28, 28)\n",
    "        X_train = X_train.reshape(-1, 784)\n",
    "        X_test = X_test.reshape(-1, 784)\n",
    "\n",
    "        # X_train = X_train[:, :100]\n",
    "        # X_test = X_test[:, :100]\n",
    "\n",
    "        if normalize:\n",
    "            X_train = X_train / 255.\n",
    "            X_test = X_test / 255.\n",
    "\n",
    "        return (X_train, y_train), (X_test, y_test), 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "backend='cpu'\n",
    "rng_seed = 0\n",
    "device_id = 0\n",
    "datatype = np.float32\n",
    "batch_size = 128\n",
    "\n",
    "# setup backend\n",
    "be = gen_backend(backend=backend,\n",
    "                 batch_size=batch_size,\n",
    "                 rng_seed=rng_seed,\n",
    "                 device_id=device_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset.\n",
    "\n",
    "  data_dir - is a path to the directory with MNIST dataset (or where to save the dataset if you have no one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_dir = './'\n",
    "(X_train, y_train), (X_test, y_test), nclass = load_mnist(path=data_dir)\n",
    "\n",
    "# setup a training set iterator\n",
    "train_set = DataIterator(X_train, y=y_train, nclass=nclass, lshape=(1, 28, 28))\n",
    "# setup a validation data set iterator\n",
    "valid_set = DataIterator(X_test, y=y_test, nclass=nclass, lshape=(1, 28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create neural network with 3 layers.\n",
    "First layer is **convolutional layer** with 2 filters of shape 1 x 5 x 5 (depth x height x width)\n",
    "Parameters of this layers are\n",
    "\n",
    "* `fshape` --- tuple of shape [depth x height x width x n_filters]\n",
    "* `init` --- initialization class object\n",
    "* `strides` --- int or dict with keys 'str_d', 'str_h', 'str_w'\n",
    "* `padding` --- int or dict with keys 'pad_d', 'pad_h', 'pad_w'\n",
    "    \n",
    "The second layer is **fully connected** layer with 100 hidden units.\n",
    "Parameters of this layer are\n",
    "\n",
    "* `n_hidden` --- number of hidden units\n",
    "* `init` --- initialization class object. It must have fill(parameter) method which assigns initial value to the parameter.\n",
    "\n",
    "The third layer is fully connected with additional input units - labels of input images.\n",
    "Additional parameters of this layer is\n",
    "\n",
    "* `n_classes` --- number of classes\n",
    "* `use_fast_weights` --- whether to use fast weights during training.\n",
    "\n",
    "Each layer also supports sparse training. The idea is that we would like to obtain sparse representation, i.e.\n",
    "not all hidden units are activated, only some small part of them. Sparsity can be controlled by the following parameters:\n",
    "    \n",
    "* `sparse_target` --- target activation ratio of hidden units\n",
    "* `sparse_cost` --- penalty for activation ratio not being close to the target\n",
    "* `sparse_damping` --- sparsity damping parameter. Defines how much activation ratio in previous iteration affects activation ratio in current iteration.\n",
    "\n",
    "And each layer has also the following parameters allowing to control training:\n",
    "\n",
    "* `persistant` --- boolean, whether to use persistant Contrastive Divergence\n",
    "* `kPCD` --- int, how much iterations to use to sample \"fantasy particles\" (in negative phase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_hidden = 100\n",
    "\n",
    "# setup weight initialization function\n",
    "init_norm = GlorotUniform()\n",
    "\n",
    "# setiup model layers\n",
    "n_filters = 2\n",
    "fshape = 5\n",
    "layers = [ConvolutionalRBMLayer(fshape=(1, fshape, fshape, n_filters),\n",
    "                                init=init_norm, strides={'str_d': 1, 'str_h': 2, 'str_w': 2},\n",
    "                                sparse_cost=0.0, sparse_damping=0.0, sparse_target=0.0,\n",
    "                                name='layer0'),\n",
    "          RBMLayer(n_hidden=100, init=init_norm,\n",
    "                   sparse_cost=0.0, sparse_damping=0.0, sparse_target=0.0,\n",
    "                   name='layer1'),\n",
    "          RBMLayerWithLabels(n_hidden=50, init=init_norm, n_classes=nclass, use_fast_weights=True, name='layer2')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Setup optimizer\n",
    "\n",
    "For each layer its own optimizer should be created.\n",
    "The only one optimization algorithm currently implemented is Gradient Descent Momentum.\n",
    "Parameters are:\n",
    "\n",
    "* `learning_rate` --- learning rate\n",
    "* `momentum` --- momentum coefficient\n",
    "* `wdecay` --- weight decay. It is a list of 3 numbers - weight decays for weights W and biases of hidden and visible units\n",
    "* `name` --- name of optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# setup optimizer\n",
    "learning_rate = 0.1\n",
    "weight_decay=[0, 0, 0]\n",
    "momentum = 0\n",
    "\n",
    "optimizer_l0 = GradientDescentMomentumRBM(0.1, 0.1, wdecay=weight_decay, name='layer0_optimizer')\n",
    "optimizer_l1 = GradientDescentMomentumRBM(0.05, 0.1, wdecay=weight_decay, name='layer1_optimizer')\n",
    "optimizer_l2 = GradientDescentMomentumRBM(0.1, 0, wdecay=weight_decay, name='layer2_optimizer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After optimizers for each layer are configured, they must be combined into one optimizer using MultiOptimizer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = MultiOptimizer({'layer0': optimizer_l0,\n",
    "                            'layer1': optimizer_l1,\n",
    "                            'layer2': optimizer_l2,\n",
    "                            'default': optimizer_l0}, name='MultiLayer_optimizer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create RBM model and configure callback. Just use standard callbacks from neon package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# initialize model object\n",
    "rbm = RBM(layers=layers)\n",
    "\n",
    "# setup standard callbacks\n",
    "callbacks = Callbacks(rbm, train_set, output_file='tmp_file')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, now we are ready to fit RBM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_epochs = 1\n",
    "\n",
    "rbm.fit(train_set, optimizer=optimizer, num_epochs=num_epochs, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate model output fprop method can be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for mb_idx, (x_val, y_val) in enumerate(valid_set):\n",
    "    hidden = rbm.fprop(x_val)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Model can be saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "serialized_model = rbm.serialize()\n",
    "\n",
    "import cPickle as pickle\n",
    "\n",
    "with open('rbm.pkl', 'wb') as save_file:\n",
    "    pickle.dump(serialized_model, save_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_rbm = RBM(layers=layers)\n",
    "new_rbm.load_weights('rbm.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
